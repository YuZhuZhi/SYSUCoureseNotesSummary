# 高等代数

## 一、正交投影(Orthogonal Projection)

### 1.正交分解定理(Decomposition)

若$W$是$R^n$的子空间，那么$R^n$中每一个向量$y$都能表示为$\bm y=proj_W\bm y+\bm z$。

其中，$proj_W\bm y$属于$W$，$z$属于$W^{\bot}$。

如果$\{u_1,u_2...u_k\}$是$W$的任意正交基，那么：

$$proj_W\bm y=\dfrac{y\cdot u_1}{u_1\cdot u_1}u_1+...+\dfrac{y\cdot u_k}{u_k\cdot u_k}u_k$$

### 2.正交投影性质

若$y\in W=Span\{u_1,u_2...u_k\}$，那么$proj_W\bm y=y$。由此得到最佳逼近定理：

### 3.最佳逼近定理(The Best Approximation)

若$W$是$R^n$的子空间，那么对于$R^n$中任一向量$y$，$proj_W\bm y$是$W$中最接近$y$的点。

### 4.单位正交基情况下的简化

若有单位正交基$\{u_1,u_2...u_k\}$，则：

$$proj_W\bm y=(y\cdot u_1)u_1+...+(y\cdot u_k)u_k$$

若记$U=[u_1,u_2...u_k]$，则：

$$proj_W\bm y=UU^Ty$$

---

## 二、施密特正交化(Gram-Schmidt Process)

### 1.正交化方法

对$R^n$中一个子空间的基$\{x_1,x_2...x_k\}$，按以下序列给出正交基：

$$\begin{cases}
    &v_1=x_1\\
    &v_2=x_2-\dfrac{x_2\cdot v_1}{v_1\cdot v_1}v_1\\
    &v_3=x_3-\dfrac{x_3\cdot v_1}{v_1\cdot v_1}v_1-\dfrac{x_3\cdot v_2}{v_2\cdot v_2}v_2\\
    &...\\
    &v_k=x_k-\dfrac{x_k\cdot v_1}{v_1\cdot v_1}v_1-...-\dfrac{x_k\cdot v_{k-1}}{v_{k-1}\cdot v_{k-1}}v_{k-1}\\
\end{cases}$$

上述向量单位化后，即可形成单位正交基/标准正交基(orthonormal basis)。

### 2.QR分解(QR Factorization)

若矩阵$A_{m\times n}$之列向量线性无关，那么$A=QR$，其中$Q_{m\times n}$的列是$Col A$的标准正交基，$R_{n\times n}$是一个上三角可逆的对角线元素为正的矩阵。

显然由于$Q$是标准正交矩阵，那么$Q^T=Q$,$R=Q^TA$

---

## 三、最小二乘问题(Least Squares)

### 1.定义

若$A_{m\times n},\ \bm b\in R^m$，那么$A\bm x=\bm b$的最小二乘解是$R^n$中的$\hat x$，使得：

$$||b-A\hat x||\leq||b-Ax||$$

总成立。

### 2.一般最小二乘问题的解

由最佳逼近定理，上述问题的解集与：

$$A^TA\bm x=A^T\bm b$$

的解集一致。

例如：

<image src="image/高等代数/6.5.1.png">

### 3.最小二乘误差(error)

$\bm b$到$A\hat x$的距离，即$||\bm b -A\hat x||$为误差。

### 4.QR分解

若$A_{m\times n}=QR$具有线性无关的列，那么$A\bm x=\bm b$的唯一最小二乘解是：

$$\hat {\bm x}=R^{-1}Q^T\bm b$$

---

## 四、对称矩阵的对角化(Diagonalization of Symmetric Matrices)

对称矩阵是$A^T=A$的方阵。

### 1.对角化过程

<image src="image/高等代数/7.1.1.png">

$A=PDP^{-1}=PDP^T$

### 2.谱分解(Spectral Decompositon)

假设$A=PDP^T$，那么：

$$A=PDP^T=[u_1,...,u_n]\begin{bmatrix}
    \lambda_1& &0\\
    &\ddots &\\
    0& &\lambda_n
\end{bmatrix}\begin{bmatrix}
    u_1^T\\
    \vdots\\
    u_n^T\\
\end{bmatrix}=[\lambda_1u_1,...,\lambda_nu_n]\begin{bmatrix}
    u_1^T\\
    \vdots\\
    u_n^T\\
\end{bmatrix}\\
=\lambda_1u_1u_1^T+...+\lambda_nu_nu_n^T$$

---

## 五、二次型(Quadratic Form)

### 1.变量代换

若$\bm x$是$R^n$中的变量向量，那么$\bm x=P\bm y$，$\bm y$是$R^n$中的新向量。因此，二次型可表示为：

$\bm x^TA\bm x=\bm y^T(P^TAP)\bm y$

$P^TAP$是新的二次型矩阵。

### 2.二次型的分类

如果一个二次型的所有特征值都是正的，那么它是正定的(positive definite)。

都是负数，它是负定的(negtive definite)。

否则，是不定的(indefinite)。

---

## 六、条件优化(Constrained Optimization)

定义如下代数：

$$m=min\{x^TAx,\ ||x||=1\},\quad M=max\{x^TAx,\ ||x||=1\}$$

### 1.

设$A$是对称矩阵，那么$m,M$分别是最小和最大的特征值，并且在分别对应的特征向量上二次型$x^TAx$取得最小最大值$m,M$。

### 2.

若在$x^Tx=1,x^Tu_1=0$的限制下，$x^TAx$的最大值是第二大的特征值，并在相应的特征向量下取得此值。这个结论可递推。

---

## 七、奇异值分解(Singular Value Decomposition)

### 1.奇异值

$A$的奇异值是$A^TA$的特征值的平方根。一般记为$\sigma_1,...\sigma_n$并且按递减顺序排列。

如果$\{\lambda_1,...,\lambda_n\}$是递减的特征值，
$\{\bm v_1,...,\bm v_n\}$是相应的标准单位正交基。若$A$有$r$个非零奇异值，那么$\{A\bm v_1,...,A\bm v_r\}$是$Col A$的一个正交基，并且$rank(A)=r$。

### 2.奇异值分解

设有$rank(A_{m\times n})=r$，那么可将其分解为$A=U\Sigma V^T$。

其中，$\Sigma_{m\times n}=\begin{bmatrix}
    D&0\\
    0&0\\
\end{bmatrix}$，$D$的对角线元素是$A$的前$r$个奇异值；$U_{m\times m}$是正交矩阵，$V_{n\times n}$是正交矩阵。

$U$的列称为$A$的左奇异向量，$V$的列称为$A$的右奇异向量。

$A_{m\times n}$奇异值的分解分三步：

(1)计算$A^TA$，将其正交对角化。

(2)计算$V,\Sigma$。将特征值降序排列，对应的单位正交特征向量集就是右奇异向量，构成$V$。之后取前$r$个非零奇异值够成$D$，再构成与$A$同行列的$Sigma$。

(3)计算$U$。$U$的前$r$列是$\{A\bm v_1,...,A\bm v_r\}$构成的单位向量，即$\bm u_k=\dfrac{1}{\sigma_k}A\bm v_k$

例：

<image src="image/高等代数/7.4.1.png">

<image src="image/高等代数/7.4.2.png">

<image src="image/高等代数/7.4.3.png">

---

## 八、习题

### (1).7.1.35

<image src="image/高等代数/7.1.35.png">

(a)
即证$B\bm x=\bm u\bm u^T\bm x=\dfrac{\bm u\cdot\bm x}{\bm u\cdot\bm u}\bm u=(\bm u\cdot\bm x)\bm u\Leftrightarrow\bm u\bm u^T\bm x=(\bm u\cdot\bm x)\bm u$
其中，$\bm u\bm u^T\bm x=\bm u(\bm u^T\bm x)=(\bm u^T\bm x)\bm u=(\bm u\cdot\bm x)\bm u$
即得证。

(b)
因$B^T=(uu^T)^T=(u^T)^T(u)^T=uu^T=B$
故$B$为对称矩阵。
因$B^2=(uu^T)(uu^T)=u(u^Tu)u^T=uu^T=B$
故$B^2=B$

(c)
即证存在$\lambda$使$B\bm u=\lambda\bm u$成立，即$\bm u\bm u^T\bm u=\lambda\bm u$
而$\bm u\bm u^T\bm u=\bm u(\bm u^T\bm u)=\bm u$
从而特征值$\lambda=1$

### (2).7.4.12

求奇异值分解：$\begin{bmatrix}
    1&1\\
    0&1\\
    -1&1\\
\end{bmatrix}$

解：

$A^TA=\begin{bmatrix}
    2 & 0\\
    0 & 3\\
\end{bmatrix}$

显然特征值为$\lambda_1=3,\lambda_1=2$，分别对应特征向量：

$\bm v_1=[0,1]^T,\bm v_2=[1,0]^T$

$V=\begin{bmatrix}
    0 & 1\\
    1 & 0\\
\end{bmatrix}$

$\Sigma=\begin{bmatrix}
    \sqrt3 & 0\\
    0 & \sqrt2\\
    0 & 0\\
\end{bmatrix}$

$Av_1=\begin{bmatrix}
    1\\
    1\\
    1\\
\end{bmatrix},
Av_2=\begin{bmatrix}
    1\\
    0\\
    -1\\
\end{bmatrix}$

故$A=\begin{bmatrix}
    \dfrac{1}{\sqrt3} & \dfrac{1}{\sqrt2} & \dfrac{1}{\sqrt6}\\
    \dfrac{1}{\sqrt3} & 0 & -\dfrac{2}{\sqrt6}\\
    \dfrac{1}{\sqrt3} & -\dfrac{1}{\sqrt2} & \dfrac{1}{\sqrt6}\\
\end{bmatrix}
\begin{bmatrix}
    \sqrt3 & 0\\
    0 & \sqrt2\\
    0 & 0\\
\end{bmatrix}
\begin{bmatrix}
    0 & 1\\
    1 & 0\\
\end{bmatrix}$

### (3).7.4.19

<image src="image/高等代数/7.4.19.png">

因$A=U\Sigma V^T$

从而$A^TA=(U\Sigma V^T)^TU\Sigma V^T=(V\Sigma^TU^T)U\Sigma V^T$

其中，$U,V$都是正交矩阵，从而：

$A^TA=V\Sigma^T(U^TU)\Sigma V^T=V(\Sigma^T\Sigma) V^T$

说明$V$将$A^TA$对角化且其为单位特征向量组。同时：

$\Sigma^T\Sigma=\begin{bmatrix}
    D & 0\\
    0 & 0\\
\end{bmatrix}\begin{bmatrix}
    D^T & 0\\
    0 & 0\\
\end{bmatrix}=\begin{bmatrix}
    DD^T & 0\\
    0 & 0\\
\end{bmatrix}$

这样，$D$对角线元素之平方就是$A^TA$的特征值，也即$D$对角线元素是$A$的奇异值。

同理，$AA^T=U\Sigma V^T(U\Sigma V^T)^T=U\Sigma V^TV\Sigma^TU^T=U(\Sigma\Sigma^T)U^T$

说明$U$是$AA^T$的单位特征向量组。

### (4).7.4.23

<image src="image/高等代数/7.4.23.png">

$A=U\Sigma V^T$

而$\Sigma=\begin{bmatrix}
    D & 0\\
    0 & 0\\
\end{bmatrix},D=\begin{bmatrix}
    \sigma_1 & 0 & \cdots & 0\\
    0 & \sigma_2 & & \vdots\\
    \vdots &  & \ddots &\\
    0 & \cdots &  & \sigma_r\\
\end{bmatrix}$

$U\Sigma=[u_1,u_2,...,u_m]\begin{bmatrix}
    D_r & 0\\
    0 & 0\\
\end{bmatrix}=\begin{bmatrix}
    \sigma_1u_1[1] & \sigma_2u_2[1] & \cdots & \sigma_ru_r[1]\\
    \sigma_1u_1[2] & \sigma_2u_2[2] & & \vdots\\
    \vdots &  & \ddots &\\
    \sigma_1u_1[?] & \cdots &  & \sigma_ru_r[?]\\
\end{bmatrix}\\=[\sigma_1u_1,\sigma_2u_2,...,\sigma_ru_r,0]$

故$A=U\Sigma V^T=[\sigma_1u_1,\sigma_2u_2,...,\sigma_ru_r,0][v_1,v_2,...,v_n]^T=\sigma_1u_1v_1+\sigma_2u_2v_2+...+\sigma_ru_rv_r$

## 九、前置

### 1.矩阵的秩

(1)$0\leq rank(A_{m\times n})\leq min\{m,n\}$

(2)$max\{R(A),R(B)\}\leq R(A,B)\leq R(A)+R(B)$

(3)$R(A+B)\leq R(A)+R(B)$

(4)$R(AB)\leq min\{R(A),R(B)\}$

(5)若$A_{m\times n}B_{n\times l}=O$，则$R(A)+R(B)\leq n$
